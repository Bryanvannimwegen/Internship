{"cells":[{"cell_type":"markdown","metadata":{"id":"LaA7bRFDwAba"},"source":["Optimized RNA-Seq Processing with Error Handling and Configuration\n","\n","This script processes RNA-Seq samples with optional truncation, alignment, quantification,\n","and coverage extraction. It supports error handling and configurable settings."]},{"cell_type":"markdown","metadata":{"id":"nC1ls6JswYYc"},"source":["#Update sra-toolkit when needed"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6506,"status":"ok","timestamp":1730283584067,"user":{"displayName":"Neal Amin","userId":"01047124843631517737"},"user_tz":-60},"id":"XCorSx6xwXme","outputId":"d5d35796-5a4d-4f57-f253-cca154ff7bde"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","Package 'sra-toolkit' is not installed, so not removed\n","0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n","--2024-10-30 10:19:40--  https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/3.1.1/sratoolkit.3.1.1-ubuntu64.tar.gz\n","Resolving ftp-trace.ncbi.nlm.nih.gov (ftp-trace.ncbi.nlm.nih.gov)... 130.14.250.12, 130.14.250.13, 2607:f220:41e:250::10, ...\n","Connecting to ftp-trace.ncbi.nlm.nih.gov (ftp-trace.ncbi.nlm.nih.gov)|130.14.250.12|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 93532905 (89M) [application/x-gzip]\n","Saving to: ‘sratoolkit.tar.gz’\n","\n","sratoolkit.tar.gz   100%[===================>]  89.20M  82.6MB/s    in 1.1s    \n","\n","2024-10-30 10:19:41 (82.6 MB/s) - ‘sratoolkit.tar.gz’ saved [93532905/93532905]\n","\n","Updated PATH: /opt/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/content/sratoolkit.3.1.1-ubuntu64/bin\n","\n","vdb-config : 3.1.1\n","\n","\n","fasterq-dump : 3.1.1\n","\n"]}],"source":["!apt-get remove --purge -y sra-toolkit\n","\n","# Download the latest SRA Toolkit for Ubuntu 64-bit\n","!wget https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/3.1.1/sratoolkit.3.1.1-ubuntu64.tar.gz -O sratoolkit.tar.gz\n","\n","\n","# Extract the downloaded tar.gz file\n","!tar -xzf sratoolkit.tar.gz\n","\n","import os\n","\n","# Identify the extracted SRA Toolkit directory\n","sratoolkit_dir = [d for d in os.listdir() if d.startswith('sratoolkit') and d.endswith('ubuntu64')][0]\n","\n","# Add the SRA Toolkit's bin directory to the PATH environment variable\n","os.environ['PATH'] += f\":/content/{sratoolkit_dir}/bin\"\n","\n","# Verify that the PATH has been updated\n","print(\"Updated PATH:\", os.environ['PATH'])\n","\n","# Check the version of vdb-config\n","!vdb-config --version\n","\n","# Check the version of fasterq-dump\n","!fasterq-dump --version"]},{"cell_type":"markdown","metadata":{"id":"uzNP_OSFwcay"},"source":["#Import packages and connect to drive"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1730283584068,"user":{"displayName":"Neal Amin","userId":"01047124843631517737"},"user_tz":-60},"id":"Q-hRIlQnfrHg"},"outputs":[],"source":["import os\n","import subprocess\n","import multiprocessing\n","import shutil\n","import bisect\n","import psutil\n","import time\n","import json"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1730283584068,"user":{"displayName":"Neal Amin","userId":"01047124843631517737"},"user_tz":-60},"id":"eyxNs5fYn70L","outputId":"0072c741-9655-4326-f2b0-6240034e5253"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of logical CPUs (threads): 96\n","Number of physical CPU cores: 48\n"]}],"source":["# Number of logical CPUs (threads)\n","logical_cpus = psutil.cpu_count(logical=True)\n","print(\"Number of logical CPUs (threads):\", logical_cpus)\n","\n","# Number of physical CPU cores\n","physical_cpus = psutil.cpu_count(logical=False)\n","print(\"Number of physical CPU cores:\", physical_cpus)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28944,"status":"ok","timestamp":1730283613008,"user":{"displayName":"Neal Amin","userId":"01047124843631517737"},"user_tz":-60},"id":"nmpMdVtH-vG5","outputId":"fe3b0bd5-4f97-4f72-d7b2-e4558daf23b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["if not os.path.exists('/content/drive'):\n","    from google.colab import drive\n","    drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"-a0-o0vZ6kG2"},"source":["# Install Necessary Tools"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":207445,"status":"ok","timestamp":1730283820451,"user":{"displayName":"Neal Amin","userId":"01047124843631517737"},"user_tz":-60},"id":"eJKlVjK-SRTc","outputId":"24b7d833-7287-43e1-c856-2df4e188fae3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["CompletedProcess(args=['chmod', '+x', '/usr/local/bin/fastp'], returncode=0)"]},"metadata":{},"execution_count":5}],"source":["# Update and install required tools\n","subprocess.run([\"apt-get\", \"update\"])\n","subprocess.run([\"apt-get\", \"install\", \"-y\", \"sra-toolkit\", \"samtools\", \"bedtools\", \"bedops\", \"libboost-all-dev\"])\n","\n","# Install STAR\n","subprocess.run([\"wget\", \"https://github.com/alexdobin/STAR/archive/refs/tags/2.7.10a.zip\"])\n","subprocess.run([\"unzip\", \"2.7.10a.zip\"])\n","subprocess.run([\"make\", \"STAR\"], cwd=\"STAR-2.7.10a/source\")\n","\n","# Install fastp\n","subprocess.run([\"wget\", \"http://opengene.org/fastp/fastp\", \"-O\", \"/usr/local/bin/fastp\"])\n","subprocess.run([\"chmod\", \"+x\", \"/usr/local/bin/fastp\"])"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":54079,"status":"ok","timestamp":1730283874527,"user":{"displayName":"Neal Amin","userId":"01047124843631517737"},"user_tz":-60},"id":"Bxau9GJqf83S"},"outputs":[],"source":["# Create a directory to store the reference genome and annotation files\n","os.makedirs(\"/content/genome\", exist_ok=True)\n","\n","# Download reference genome\n","subprocess.run([\n","    \"wget\",\n","    \"-O\", \"/content/genome/genome.fa.gz\",\n","    \"ftp://ftp.ensembl.org/pub/release-101/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz\"\n","])\n","# Download annotation file\n","subprocess.run([\n","    \"wget\",\n","    \"-O\", \"/content/genome/annotation.gtf.gz\",\n","    \"ftp://ftp.ensembl.org/pub/release-101/gtf/homo_sapiens/Homo_sapiens.GRCh38.101.gtf.gz\"\n","])\n","\n","# Unzip the files\n","subprocess.run([\"gunzip\", \"/content/genome/genome.fa.gz\"])\n","subprocess.run([\"gunzip\", \"/content/genome/annotation.gtf.gz\"])\n","\n","# Define paths to genome and annotation files\n","genome_fasta = \"/content/genome/genome.fa\"\n","annotation_gtf = \"/content/genome/annotation.gtf\""]},{"cell_type":"markdown","metadata":{"id":"fWzDctClpv_P"},"source":["# Install Salmon and Build Index"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":231751,"status":"ok","timestamp":1730284106275,"user":{"displayName":"Neal Amin","userId":"01047124843631517737"},"user_tz":-60},"id":"5Evx2GGcpve6","outputId":"88631b6a-5443-461f-9001-0a55f6771aa8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["CompletedProcess(args=['salmon', 'index', '-t', '/content/genome/Homo_sapiens.GRCh38.cdna.all.fa', '-i', '/content/genome/salmon_index', '-k', '31'], returncode=0)"]},"metadata":{},"execution_count":7}],"source":["# Install Salmon\n","subprocess.run([\"wget\", \"https://github.com/COMBINE-lab/salmon/releases/download/v1.9.0/salmon-1.9.0_linux_x86_64.tar.gz\"])\n","subprocess.run([\"tar\", \"-xzf\", \"salmon-1.9.0_linux_x86_64.tar.gz\"])\n","subprocess.run([\"cp\", \"salmon-1.9.0_linux_x86_64/bin/salmon\", \"/usr/local/bin/\"])\n","\n","# Download cDNA sequences (transcriptome)\n","subprocess.run([\n","    \"wget\",\n","    \"-O\", \"/content/genome/Homo_sapiens.GRCh38.cdna.all.fa.gz\",\n","    \"ftp://ftp.ensembl.org/pub/release-101/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz\"\n","])\n","\n","# Unzip the file\n","subprocess.run([\"gunzip\", \"/content/genome/Homo_sapiens.GRCh38.cdna.all.fa.gz\"])\n","\n","# Build Salmon index\n","subprocess.run([\n","    \"salmon\",\n","    \"index\",\n","    \"-t\", \"/content/genome/Homo_sapiens.GRCh38.cdna.all.fa\",\n","    \"-i\", \"/content/genome/salmon_index\",\n","    \"-k\", \"31\"  # Default k-mer size is 31\n","])"]},{"cell_type":"markdown","metadata":{"id":"-e9JcgEJ_GRU"},"source":["#Picardtools setup"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":48656,"status":"ok","timestamp":1730284154928,"user":{"displayName":"Neal Amin","userId":"01047124843631517737"},"user_tz":-60},"id":"3kc_RNKQ_F_J","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8094bf06-5a4f-48cf-e563-46d3e4b47d8b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["CompletedProcess(args='awk \\'{print $1\"\\\\t\"($2+1)\"\\\\t\"$3\"\\\\t+\\\\tRibosomalRNA\"}\\' /content/genome/rRNA_merged.bed >> /content/genome/ribosomal_intervals.list', returncode=0)"]},"metadata":{},"execution_count":8}],"source":["# Install Picard Tools\n","subprocess.run([\n","    \"wget\", \"https://github.com/broadinstitute/picard/releases/download/3.2.0/picard.jar\", \"-O\", \"/content/picard.jar\"\n","])\n","\n","# Install Java\n","subprocess.run([\"apt-get\", \"install\", \"-y\", \"openjdk-17-jdk-headless\"])\n","\n","os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-17-openjdk-amd64'\n","os.environ['PATH'] = os.environ['JAVA_HOME'] + '/bin:' + os.environ['PATH']\n","\n","# Download gtfToGenePred from UCSC\n","subprocess.run(\"wget http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/gtfToGenePred -O /usr/local/bin/gtfToGenePred\", shell=True)\n","subprocess.run(\"chmod +x /usr/local/bin/gtfToGenePred\", shell=True)\n","\n","# Generate refFlat file required by Picard\n","subprocess.run(\"gtfToGenePred /content/genome/annotation.gtf /content/genome/genes.genePred\", shell=True)\n","\n","# Corrected awk command for refFlat.txt\n","subprocess.run(\"awk 'BEGIN {OFS=\\\"\\\\t\\\"} {print $12, $1, $2, $3, $4, $5, $6, $7, $8, $9, $10, \\\".\\\", \\\".\\\"}' /content/genome/genes.genePred > /content/genome/refFlat.txt\", shell=True)\n","subprocess.run(\"awk '{OFS=\\\"\\\\t\\\"; gene=($12 == \\\".\\\" || $12 == \\\"\\\") ? $1 : $12; print gene, $0}' /content/genome/genes.genePred > /content/genome/refFlat.txt\", shell=True)\n","\n","# Generate ribosomal intervals\n","subprocess.run(\"grep -i \\\"rRNA\\\" /content/genome/annotation.gtf > /content/genome/rRNA.gtf\", shell=True)\n","subprocess.run(\"gtfToGenePred /content/genome/rRNA.gtf /content/genome/rRNA.genePred\", shell=True)\n","\n","# Convert GenePred to BED\n","subprocess.run(\"awk '{print $2\\\"\\\\t\\\"$4\\\"\\\\t\\\"$5}' /content/genome/rRNA.genePred > /content/genome/rRNA.bed\", shell=True)\n","\n","# Sort the BED file\n","subprocess.run(\"sort -k1,1 -k2,2n /content/genome/rRNA.bed > /content/genome/rRNA_sorted.bed\", shell=True)\n","\n","# Merge overlapping intervals\n","subprocess.run(\"bedtools merge -i /content/genome/rRNA_sorted.bed > /content/genome/rRNA_merged.bed\", shell=True)\n","\n","# Index the genome FASTA to get chromosome lengths\n","subprocess.run(\"samtools faidx /content/genome/genome.fa\", shell=True)\n","\n","# Start the IntervalList with the @HD header\n","subprocess.run(\"echo \\\"@HD\\tVN:1.0\\tSO:coordinate\\\" > /content/genome/ribosomal_intervals.list\", shell=True)\n","\n","# Add @SQ headers for each chromosome based on genome.fa.fai\n","subprocess.run(\"awk '{print \\\"@SQ\\\\tSN:\\\"$1\\\"\\\\tLN:\\\"$2}' /content/genome/genome.fa.fai >> /content/genome/ribosomal_intervals.list\", shell=True)\n","\n","# Convert merged BED to IntervalList entries and append to the list\n","subprocess.run(\"awk '{print $1\\\"\\\\t\\\"($2+1)\\\"\\\\t\\\"$3\\\"\\\\t+\\\\tRibosomalRNA\\\"}' /content/genome/rRNA_merged.bed >> /content/genome/ribosomal_intervals.list\", shell=True)\n"]},{"cell_type":"markdown","metadata":{"id":"mWmDjKQJ_Izo"},"source":["#STAR Index creation"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":2863852,"status":"ok","timestamp":1730287947370,"user":{"displayName":"Neal Amin","userId":"01047124843631517737"},"user_tz":-60},"id":"hKzlvF2JgMDR","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ee0e6d64-e92b-4c2e-c937-6deca4816a3f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["CompletedProcess(args=['./STAR-2.7.10a/source/STAR', '--runThreadN', '96', '--runMode', 'genomeGenerate', '--genomeDir', '/content/star_genome_index', '--genomeFastaFiles', '/content/genome/genome.fa', '--sjdbGTFfile', '/content/genome/annotation.gtf'], returncode=0)"]},"metadata":{},"execution_count":10}],"source":["# Build index\n","subprocess.run([\n","    \"./STAR-2.7.10a/source/STAR\",\n","    \"--runThreadN\", \"96\",\n","    \"--runMode\", \"genomeGenerate\",\n","    \"--genomeDir\", \"/content/star_genome_index\",\n","    \"--genomeFastaFiles\", genome_fasta,\n","    \"--sjdbGTFfile\", annotation_gtf\n","])"]},{"cell_type":"markdown","metadata":{"id":"oGS3qjb3gXOV"},"source":["# Define Functions for Coverage Processing"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1730287947371,"user":{"displayName":"Neal Amin","userId":"01047124843631517737"},"user_tz":-60},"id":"uEL34c47gSxy"},"outputs":[],"source":["def load_sorted_coverage(filepath):\n","    coverage_dict = {}\n","    with open(filepath, \"r\") as file:\n","        for line in file:\n","            chrom, pos, coverage = line.strip().split()\n","            pos = int(pos) - 1  # Convert to 0-based position\n","            if chrom not in coverage_dict:\n","                coverage_dict[chrom] = []\n","            coverage_dict[chrom].append((pos, int(coverage)))\n","    return coverage_dict\n","\n","def filter_sequences_by_hard_min_coverage(input_file, output_file, hard_min_coverage):\n","    with open(input_file, \"r\") as infile, open(output_file, \"w\") as outfile:\n","        header, sequence, coverage_str = None, None, None\n","        for line in infile:\n","            line = line.strip()\n","            if line.startswith(\">\"):\n","                if header and sequence and coverage_str:\n","                    coverage_values = list(map(int, coverage_str.split(',')))\n","                    if all(c >= hard_min_coverage for c in coverage_values):\n","                        outfile.write(f\"{header}\\n{sequence}\\n{coverage_str}\\n\")\n","                header, sequence, coverage_str = line, None, None\n","            elif not sequence:\n","                sequence = line\n","            else:\n","                coverage_str = line\n","        if header and sequence and coverage_str:\n","            coverage_values = list(map(int, coverage_str.split(',')))\n","            if all(c >= hard_min_coverage for c in coverage_values):\n","                outfile.write(f\"{header}\\n{sequence}\\n{coverage_str}\\n\")\n","    print(f\"Filtered sequences saved to {output_file}\")\n","\n","def adjust_fasta_headers(input_fasta, corrected_fasta):\n","    \"\"\"\n","    Adjusts the start position in FASTA headers by incrementing it by 1.\n","    \"\"\"\n","    try:\n","        with open(input_fasta, 'r') as infile, open(corrected_fasta, 'w') as outfile:\n","            for line in infile:\n","                line = line.rstrip('\\n')\n","                if line.startswith('>'):\n","                    # Remove the initial '>' and split the header\n","                    header = line[1:].strip()\n","                    try:\n","                        chrom, positions = header.split(':')\n","                        start, end = positions.split('-')\n","                        new_start = int(start) + 1\n","                        # Construct the new header\n","                        new_header = f\">{chrom}:{new_start}-{end}\"\n","                        outfile.write(f\"{new_header}\\n\")\n","                    except ValueError:\n","                        print(f\"Warning: Malformed header skipped: {line.strip()}\")\n","                        continue  # Skip malformed headers\n","                else:\n","                    # Write sequence and coverage lines unchanged\n","                    outfile.write(f\"{line}\\n\")\n","        print(f\"Adjusted headers saved to {corrected_fasta}\")\n","    except FileNotFoundError:\n","        print(f\"Error: The file {input_fasta} does not exist.\")\n","    except Exception as e:\n","        print(f\"An unexpected error occurred: {e}\")\n","\n","def filter_fasta_by_chromosome(input_fasta, output_fasta, excluded_chromosomes):\n","    \"\"\"\n","    Filters out sequences from the specified chromosomes in a FASTA file.\n","\n","    Args:\n","        input_fasta (str): Path to the input FASTA file.\n","        output_fasta (str): Path to the output FASTA file with filtered sequences.\n","        excluded_chromosomes (list): List of chromosome identifiers to exclude.\n","    \"\"\"\n","    with open(input_fasta, 'r') as infile, open(output_fasta, 'w') as outfile:\n","        write_sequence = False\n","        for line in infile:\n","            if line.startswith('>'):\n","                # Get the chromosome from the header\n","                header = line.strip()\n","                chrom_region = header[1:]  # Remove '>'\n","                if ':' in chrom_region:\n","                    chrom, _ = chrom_region.split(':', 1)\n","                else:\n","                    chrom = chrom_region\n","                if chrom in excluded_chromosomes:\n","                    write_sequence = False\n","                else:\n","                    write_sequence = True\n","                    outfile.write(line + '\\n')\n","            else:\n","                if write_sequence:\n","                    outfile.write(line)\n","\n","def fasta_to_bed(fasta_file, bed_file):\n","    \"\"\"\n","    Convert the FASTA headers (regions) into a BED file.\n","    \"\"\"\n","    with open(fasta_file, \"r\") as infile, open(bed_file, \"w\") as outfile:\n","        for line in infile:\n","            line = line.strip()\n","            if line.startswith(\">\"):\n","                # Parse the chromosome and region from the header (e.g., \">1:1000-2000\")\n","                chrom_region = line[1:].strip()\n","                if ':' in chrom_region:\n","                    chrom, region = chrom_region.split(\":\")\n","                    start, end = region.split(\"-\")\n","                    outfile.write(f\"{chrom}\\t{int(start) - 1}\\t{end}\\n\")  # Convert to 0-based start for BED\n","    print(f\"Regions have been converted to BED format and saved to {bed_file}\")\n"]},{"cell_type":"markdown","metadata":{"id":"6C8o7yGlgcLD"},"source":["# Define the Per-Sample Processing Function"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":316,"status":"ok","timestamp":1730293583889,"user":{"displayName":"Neal Amin","userId":"01047124843631517737"},"user_tz":-60},"id":"Xz5aCc6mt-vm"},"outputs":[],"source":["def process_sample(sample, adjusted_read_length, library_layout, filter_sequences=False, truncate=False, truncation_range=(50, 100), truncation_step=10,\n","                   run_star=True, run_picard=True, run_salmon=True, threads_per_sample=16, regions_fasta_file='path',\n","                   drive_metrics_dir='/content/drive/My Drive/Colab/Bryan_Colab'):\n","    \"\"\"\n","    Processes a single RNA-Seq sample by performing the following steps:\n","    If truncate is True:\n","        For each truncation length:\n","            1. Truncate reads to different lengths using fastp.\n","            2. Aligns truncated reads with STAR.\n","            3. Indexes the BAM file with samtools.\n","            4. Collects RNA-Seq metrics using Picard.\n","            5. Runs Salmon quantification.\n","            6. Uses regions from another dataset, excluding chromosome X sequences.\n","            7. Extracts coverage values and processes sequences.\n","            8. Copies the metrics and processed data to Google Drive.\n","            9. Deletes intermediate files to free up storage.\n","    If truncate is False:\n","        1. Use fastp for adapter removal only.\n","        2. Proceed with the rest of the processing steps without truncation.\n","        3. Extract read length information from the fastp report.\n","\n","    Args:\n","        sample (str): The sample name (e.g., \"SRR10072432\").\n","        adjusted_read_length (float): Adjusted read length after considering paired-end reads.\n","        library_layout (str): 'PAIRED' or 'SINGLE' indicating the library layout.\n","        filter_sequences (bool): Flag to indicate whether to filter sequences.\n","        truncate (bool): Flag to control whether to perform truncation.\n","        truncation_range (tuple): Tuple specifying the start and end of truncation lengths.\n","        truncation_step (int): Step size for truncation lengths.\n","        run_star (bool): Flag to control whether to run STAR alignment.\n","        run_picard (bool): Flag to control whether to run Picard metrics collection.\n","        run_salmon (bool): Flag to control whether to run Salmon quantification.\n","        threads_per_sample (int): Number of threads to use per sample.\n","        regions_fasta_file (str): Path to the regions FASTA file.\n","        drive_metrics_dir (str): Directory on Google Drive to save outputs.\n","    \"\"\"\n","    print(f\"Processing sample: {sample}\\n\")\n","    start_time = time.time()\n","\n","    # Set Java environment variables within the child process\n","    os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-17-openjdk-amd64'\n","    os.environ['PATH'] = os.environ['JAVA_HOME'] + '/bin:' + os.environ['PATH']\n","\n","    # Create a sample-specific directory\n","    sample_dir = f\"/content/{sample}\"\n","    if not os.path.exists(sample_dir):\n","        os.makedirs(sample_dir)\n","\n","    # Download Sample Data from SRA using fastq-dump\n","    try:\n","        subprocess.run([\n","            \"fastq-dump\",\n","            \"--split-files\",\n","            \"--gzip\",\n","            \"-O\", sample_dir,\n","            sample  # Use the sample name directly\n","        ], check=True)\n","    except subprocess.CalledProcessError as e:\n","        print(f\"fastq-dump failed for {sample}: {e}\")\n","        return  # Skip processing this sample\n","\n","    # Determine input FASTQ file based on library layout\n","    if library_layout == 'PAIRED':\n","        input_fastq = f\"{sample_dir}/{sample}_1.fastq.gz\"\n","    else:\n","        input_fastq = f\"{sample_dir}/{sample}.fastq.gz\"\n","\n","    # Check if input_fastq exists\n","    if not os.path.exists(input_fastq):\n","        print(f\"Input FASTQ file not found for {sample}\")\n","        return  # Skip processing this sample\n","\n","    # Verify downloaded files\n","    print(f\"Downloaded files for {sample}:\")\n","    subprocess.run([\"ls\", sample_dir])\n","\n","    # Define paths to genome and annotation files\n","    genome_fasta = \"/content/genome/genome.fa\"\n","    annotation_gtf = \"/content/genome/annotation.gtf\"\n","\n","    # Define paths to Picard input and output\n","    ref_flat = \"/content/genome/refFlat.txt\"\n","    ribosomal_intervals = \"/content/genome/ribosomal_intervals.list\"\n","\n","    # Determine truncation lengths\n","    if truncate:\n","        truncation_lengths = list(range(truncation_range[0], truncation_range[1]+1, truncation_step))\n","    else:\n","        truncation_lengths = [None]  # No truncation\n","\n","    for length in truncation_lengths:\n","        try:\n","            if length is not None:\n","                # Truncation Mode: Run fastp with truncation\n","                length_str = str(length)\n","                print(f\"\\nProcessing truncation length: {length_str}\")\n","\n","                # Define the output FASTQ file for truncated reads\n","                fastq_output = f\"{sample_dir}/{sample}_truncated_{length_str}.fastq.gz\"\n","\n","                # Define the output JSON report file\n","                json_report = f\"{sample_dir}/fastp_report_{length_str}.json\"\n","\n","                # Run fastp with truncation and adapter removal\n","                fastp_cmd = [\n","                    \"fastp\",\n","                    \"-i\", input_fastq,\n","                    \"-o\", fastq_output,\n","                    \"--cut_right\",\n","                    \"--max_len1\", length_str,\n","                    \"--length_required\", length_str,\n","                    \"-j\", json_report,\n","                ]\n","            else:\n","                # Non-Truncation Mode: Run fastp for adapter removal only\n","                length_str = str(int(adjusted_read_length))\n","                print(f\"\\nProcessing original sample without truncation, adjusted read length: {length_str}\")\n","\n","                # Define the output FASTQ file for adapter-removed reads\n","                fastq_output = f\"{sample_dir}/{sample}_fastp_processed.fastq.gz\"\n","\n","                # Define the output JSON report file\n","                json_report = f\"{sample_dir}/fastp_report_{length_str}.json\"\n","\n","                # Run fastp without truncation, only adapter removal\n","                fastp_cmd = [\n","                    \"fastp\",\n","                    \"-i\", input_fastq,\n","                    \"-o\", fastq_output,\n","                    \"-j\", json_report,\n","                ]\n","\n","            # Execute fastp and capture its output\n","            try:\n","                result = subprocess.run(\n","                    fastp_cmd,\n","                    check=True,\n","                    stdout=subprocess.PIPE,\n","                    stderr=subprocess.PIPE,\n","                    text=True\n","                )\n","                print(f\"fastp output for {sample} at length {length_str}:\\n{result.stdout}\")\n","                if result.stderr:\n","                    print(f\"fastp stderr for {sample} at length {length_str}:\\n{result.stderr}\")\n","            except subprocess.CalledProcessError as e:\n","                print(f\"fastp failed for {sample} at length {length_str}: {e}\")\n","                print(f\"fastp stdout: {e.stdout}\")\n","                print(f\"fastp stderr: {e.stderr}\")\n","                continue  # Skip to next length\n","\n","            # Set fastq_input to the output of fastp\n","            fastq_input = fastq_output\n","\n","            # After running fastp and before proceeding to alignment\n","            try:\n","                # Read the fastp JSON report\n","                with open(json_report, 'r') as f:\n","                    fastp_report = json.load(f)\n","\n","                # Extract the read length from total_cycles\n","                read_length = fastp_report['read1_after_filtering']['total_cycles']\n","\n","                # Extract total_bases after filtering\n","                total_bases = fastp_report['summary']['after_filtering']['total_bases']\n","\n","                # Print or save the depth information\n","                print(f\"Total bases (depth) after filtering: {total_bases}\")\n","\n","                # Calculate depth in Mbases for readability\n","                depth_mb = round(total_bases / 1e6, 2)\n","                print(f\"Depth in Mbases: {depth_mb} Mb\")\n","\n","                # Save read length and depth information to a file\n","                read_length_info_file = f\"{sample_dir}/{sample}_{length_str}_read_length_info.txt\"\n","                with open(read_length_info_file, 'w') as f:\n","                    f.write(f\"Read length after filtering: {read_length}\\n\")\n","                    f.write(f\"Total bases (depth) after filtering: {total_bases}\\n\")\n","                    f.write(f\"Depth in Mbases: {depth_mb} Mb\\n\")\n","\n","                # Copy the read length info to Google Drive\n","                #drive_read_length_info_file = os.path.join(drive_metrics_dir, f'{sample}_{length_str}_read_length_info.txt')\n","                #shutil.copy(read_length_info_file, drive_read_length_info_file)\n","                #print(f\"Copied read length info to Google Drive: {drive_read_length_info_file}\")\n","            except Exception as e:\n","                print(f\"Failed to extract read length information for {sample} at length {length_str}: {e}\")\n","\n","            bam_file = None\n","\n","            # Align reads with STAR\n","            if run_star:\n","                try:\n","                    # Align reads with STAR without passing Read Group lines\n","                    cpu_count = psutil.cpu_count(logical=True)\n","                    threads = min(threads_per_sample, cpu_count)\n","                    subprocess.run([\n","                        \"./STAR-2.7.10a/source/STAR\",\n","                        \"--runThreadN\", str(threads),\n","                        \"--genomeDir\", \"/content/star_genome_index\",\n","                        \"--readFilesIn\", fastq_input,\n","                        \"--outFileNamePrefix\", f\"{sample_dir}/{sample}_output_STAR_{length_str}\",\n","                        \"--readFilesCommand\", \"zcat\",\n","                        \"--outSAMtype\", \"BAM\", \"SortedByCoordinate\"\n","                    ], check=True)\n","                    # Path to the aligned BAM file\n","                    bam_file = f\"{sample_dir}/{sample}_output_STAR_{length_str}Aligned.sortedByCoord.out.bam\"\n","                    # Index the BAM file using samtools\n","                    subprocess.run([\n","                        \"samtools\", \"index\", bam_file\n","                    ], check=True)\n","                except subprocess.CalledProcessError as e:\n","                    print(f\"STAR alignment failed for {sample} at length {length_str}: {e}\")\n","                    continue  # Skip to next length\n","            else:\n","                print(f\"Skipping STAR alignment for {sample} at length {length_str}\")\n","\n","            # Run Picard CollectRnaSeqMetrics\n","            if run_picard and bam_file:\n","                try:\n","                    metrics_output = f\"{sample_dir}/{sample}_{length_str}_Picard_RNA_Metrics.txt\"\n","                    chart_output = f\"{sample_dir}/{sample}_{length_str}_Picard_RNA_Metrics.pdf\"\n","                    picard_cmd = [\n","                        \"java\", \"-jar\", \"/content/picard.jar\", \"CollectRnaSeqMetrics\",\n","                        f\"I={bam_file}\",\n","                        f\"O={metrics_output}\",\n","                        f\"REF_FLAT={ref_flat}\",\n","                        f\"RIBOSOMAL_INTERVALS={ribosomal_intervals}\",\n","                        \"STRAND=NONE\",\n","                        f\"CHART_OUTPUT={chart_output}\",\n","                        f\"R={genome_fasta}\"\n","                    ]\n","                    result = subprocess.run(picard_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n","                    if result.returncode != 0:\n","                        print(f\"Picard CollectRnaSeqMetrics failed for {sample} at length {length_str}\")\n","                        print(f\"Picard stdout: {result.stdout}\")\n","                        print(f\"Picard stderr: {result.stderr}\")\n","                        continue\n","                    else:\n","                        print(f\"Picard CollectRnaSeqMetrics completed successfully for {sample} at length {length_str}\")\n","                    # Copy the metrics files to Google Drive\n","                    drive_metrics_file = os.path.join(drive_metrics_dir, f'{sample}_{length_str}_Picard_RNA_Metrics.txt')\n","                    drive_metrics_file_pdf = os.path.join(drive_metrics_dir, f'{sample}_{length_str}_Picard_RNA_Metrics.pdf')\n","                    shutil.copy(metrics_output, drive_metrics_file)\n","                    shutil.copy(chart_output, drive_metrics_file_pdf)\n","                    print(f\"Copied RNA-Seq metrics to Google Drive: {drive_metrics_file}\")\n","                except Exception as e:\n","                    print(f\"Picard CollectRnaSeqMetrics failed for {sample} at length {length_str}: {e}\")\n","                    continue  # Skip to next length\n","            else:\n","                print(f\"Skipping Picard CollectRnaSeqMetrics for {sample} at length {length_str}\")\n","\n","            # Run Salmon Quantification\n","            if run_salmon:\n","                try:\n","                    salmon_output_dir = f\"{sample_dir}/salmon_quant_{length_str}\"\n","                    subprocess.run([\n","                        \"salmon\",\n","                        \"quant\",\n","                        \"-i\", \"/content/genome/salmon_index\",\n","                        \"-l\", \"A\",  # Automatically detect library type\n","                        \"-r\", fastq_input,\n","                        \"-p\", str(threads_per_sample),\n","                        \"-o\", salmon_output_dir\n","                    ], check=True)\n","                    print(f\"Salmon quantification completed for {sample} at length {length_str}\")\n","                    # Copy Salmon outputs to Google Drive\n","                    drive_salmon_output_dir = os.path.join(drive_metrics_dir, f'{sample}_salmon_quant_{length_str}')\n","                    shutil.copytree(salmon_output_dir, drive_salmon_output_dir)\n","                    print(f\"Copied Salmon quantification results to Google Drive: {drive_salmon_output_dir}\")\n","                except subprocess.CalledProcessError as e:\n","                    print(f\"Salmon quantification failed for {sample} at length {length_str}: {e}\")\n","                    continue  # Skip to next length\n","            else:\n","                print(f\"Skipping Salmon quantification for {sample} at length {length_str}\")\n","\n","            # If bam_file exists, proceed with coverage extraction\n","            if bam_file:\n","                try:\n","                    \"\"\"# Use Regions from Another Dataset, Excluding Chromosome X\"\"\"\n","                    # Use regions_fasta_file parameter\n","                    # Exclude sequences from chromosome X\n","                    excluded_chromosomes = ['X']\n","                    filtered_regions_fasta_file = f\"{sample_dir}/regions_from_other_dataset_filtered.fa\"\n","                    filter_fasta_by_chromosome(regions_fasta_file, filtered_regions_fasta_file, excluded_chromosomes)\n","\n","                    # Convert the regions from the filtered dataset to BED\n","                    bed_file = f\"{sample_dir}/regions_from_other_dataset_{length_str}.bed\"\n","                    fasta_to_bed(filtered_regions_fasta_file, bed_file)\n","\n","                    # Extract sequences from genome for these regions\n","                    sequences_file = f\"{sample_dir}/extracted_sequences_{length_str}.fa\"\n","                    subprocess.run([\n","                        \"bedtools\", \"getfasta\",\n","                        \"-fi\", genome_fasta,\n","                        \"-bed\", bed_file,\n","                        \"-fo\", sequences_file\n","                    ], check=True)\n","\n","                    \"\"\"# Coverage Extraction and Sequence Processing\"\"\"\n","                    # Define output coverage file\n","                    coverage_output = f\"{sample_dir}/coverage_per_base_{length_str}.samtools.bed\"\n","                    # Calculate coverage using the BAM file for the current length\n","                    with open(coverage_output, \"w\") as cov_out:\n","                        subprocess.run([\"samtools\", \"depth\", \"-b\", bed_file, bam_file], stdout=cov_out, check=True)\n","\n","                    # Sort the coverage data\n","                    sorted_coverage_output = f\"{sample_dir}/sorted_coverage_per_base_{length_str}.samtools.bed\"\n","                    with open(sorted_coverage_output, \"w\") as sorted_cov_out:\n","                        subprocess.run([\"sort\", \"-k1,1\", \"-k2,2n\", coverage_output], stdout=sorted_cov_out, check=True)\n","\n","                    # Load the coverage data\n","                    coverage_data = load_sorted_coverage(sorted_coverage_output)\n","\n","                    # Define output file for sequences with coverage\n","                    output_file = f\"{sample_dir}/{sample}_{length_str}_enhanced_sequences_with_coverage.fa\"\n","\n","                    # Open the sequences file and process\n","                    with open(sequences_file, \"r\") as fasta_file, open(output_file, \"w\") as outfile:\n","                        header = None\n","                        sequence = \"\"\n","\n","                        for line in fasta_file:\n","                            line = line.strip()\n","                            if line.startswith(\">\"):\n","                                if header and sequence:\n","                                    chrom_region = header[1:]\n","                                    if ':' in chrom_region:\n","                                        chrom, region = chrom_region.split(\":\")\n","                                        start, end = map(int, region.split(\"-\"))\n","                                    else:\n","                                        chrom = chrom_region\n","                                        start, end = 0, len(sequence)\n","                                    coverage_list = coverage_data.get(chrom, [])\n","                                    start_positions = [pos for pos, cov in coverage_list]\n","                                    idx = bisect.bisect_left(start_positions, start)\n","\n","                                    coverage_values = []\n","                                    for i in range(len(sequence)):\n","                                        current_position = start + i\n","                                        if idx < len(coverage_list) and coverage_list[idx][0] == current_position:\n","                                            coverage_values.append(coverage_list[idx][1])\n","                                            idx += 1\n","                                        else:\n","                                            coverage_values.append(0)\n","                                    coverage_str = ','.join(map(str, coverage_values))\n","                                    outfile.write(f\"{header}\\n{sequence}\\n{coverage_str}\\n\")\n","                                header = line\n","                                sequence = \"\"\n","                            else:\n","                                sequence += line\n","\n","                        # Process the last sequence\n","                        if header and sequence:\n","                            chrom_region = header[1:]\n","                            if ':' in chrom_region:\n","                                chrom, region = chrom_region.split(\":\")\n","                                start, end = map(int, region.split(\"-\"))\n","                            else:\n","                                chrom = chrom_region\n","                                start, end = 0, len(sequence)\n","                            coverage_list = coverage_data.get(chrom, [])\n","                            start_positions = [pos for pos, cov in coverage_list]\n","                            idx = bisect.bisect_left(start_positions, start)\n","\n","                            coverage_values = []\n","                            for i in range(len(sequence)):\n","                                current_position = start + i\n","                                if idx < len(coverage_list) and coverage_list[idx][0] == current_position:\n","                                    coverage_values.append(coverage_list[idx][1])\n","                                    idx += 1\n","                                else:\n","                                    coverage_values.append(0)\n","                            coverage_str = ','.join(map(str, coverage_values))\n","                            outfile.write(f\"{header}\\n{sequence}\\n{coverage_str}\\n\")\n","                    print(f\"Sequences with coverage data saved to {output_file}\")\n","\n","                    if filter_sequences:\n","                        # Filter sequences based on hard minimum coverage\n","                        filtered_output_file = f\"{sample_dir}/{sample}_{length_str}_filtered_high_quality_sequences.fa\"\n","                        filter_sequences_by_hard_min_coverage(output_file, filtered_output_file, hard_min_coverage=10)\n","\n","                        # Adjust the headers and save to a new file\n","                        corrected_fasta = filtered_output_file.replace('.fa', '_corrected.fa')\n","\n","                        # Adjust the headers\n","                        adjust_fasta_headers(filtered_output_file, corrected_fasta)\n","\n","                        # Define the destination path on Google Drive with depth in filename\n","                        drive_corrected_file = os.path.join(\n","                            drive_metrics_dir,\n","                            f'{sample}_readlength_{length_str}_depth_{depth_mb}Mb.fa'\n","                        )\n","                    else:\n","                        # No filtering; use the original output_file\n","                        corrected_fasta = output_file.replace('.fa', '_corrected.fa')\n","\n","                        # Adjust the headers\n","                        adjust_fasta_headers(output_file, corrected_fasta)\n","\n","                        # Define the destination path on Google Drive with depth in filename\n","                        drive_corrected_file = os.path.join(\n","                            drive_metrics_dir,\n","                            f'{sample}_readlength_{length_str}_depth_{depth_mb}Mb.fa'\n","                        )\n","\n","                    # Copy the corrected FASTA file to Google Drive\n","                    shutil.copy(corrected_fasta, drive_corrected_file)\n","                    print(f\"Copied corrected FASTA to Google Drive: {drive_corrected_file}\")\n","                except Exception as e:\n","                    print(f\"Error during coverage extraction and sequence processing for {sample} at length {length_str}: {e}\")\n","                    continue  # Skip to next length\n","            else:\n","                print(f\"No BAM file available, skipping coverage extraction and sequence processing for {sample} at length {length_str}\")\n","\n","            # Delete intermediate files to free up storage\n","            try:\n","                if length is not None:\n","                    os.remove(fastq_input)  # Remove truncated or processed FASTQ\n","                if bam_file:\n","                    os.remove(bam_file)\n","                    os.remove(f\"{bam_file}.bai\")\n","                if run_picard and bam_file:\n","                    os.remove(metrics_output)\n","                    os.remove(chart_output)\n","                if 'coverage_output' in locals():\n","                    os.remove(coverage_output)\n","                if 'sorted_coverage_output' in locals():\n","                    os.remove(sorted_coverage_output)\n","                if 'output_file' in locals():\n","                    os.remove(output_file)\n","                if 'corrected_fasta' in locals():\n","                    os.remove(corrected_fasta)\n","                if 'sequences_file' in locals():\n","                    os.remove(sequences_file)\n","                if 'bed_file' in locals():\n","                    os.remove(bed_file)\n","                if 'filtered_regions_fasta_file' in locals():\n","                    os.remove(filtered_regions_fasta_file)\n","                if 'read_length_info_file' in locals():\n","                    os.remove(read_length_info_file)\n","                # Do not delete Salmon outputs if you need them\n","                # Uncomment the next line if you want to delete Salmon outputs\n","                # if run_salmon and 'salmon_output_dir' in locals():\n","                #     shutil.rmtree(salmon_output_dir)\n","                print(f\"Deleted intermediate files for length {length_str}\")\n","            except Exception as e:\n","                print(f\"Error deleting intermediate files for length {length_str}: {e}\")\n","\n","        except Exception as e:\n","            print(f\"Processing failed for {sample} at length {length_str}: {e}\")\n","            continue  # Skip to next length\n","\n","    # Delete the original FASTQ files\n","    try:\n","        if library_layout == 'PAIRED':\n","            os.remove(f\"{sample_dir}/{sample}_1.fastq.gz\")\n","            os.remove(f\"{sample_dir}/{sample}_2.fastq.gz\")\n","        else:\n","            os.remove(f\"{sample_dir}/{sample}.fastq.gz\")\n","        print(f\"Deleted original FASTQ files for {sample}\")\n","    except Exception as e:\n","        print(f\"Error deleting original FASTQ files for {sample}: {e}\")\n","\n","    # Delete the sample directory to free up storage\n","    try:\n","        shutil.rmtree(sample_dir)\n","        print(f\"Deleted sample directory {sample_dir} to free up storage.\")\n","    except Exception as e:\n","        print(f\"Error deleting sample directory {sample_dir}: {e}\")\n","\n","    end_time = time.time()\n","    print(f\"\\nAll processing steps completed for {sample}.\")\n","    print(f\"Data preprocessing took {end_time - start_time:.2f} seconds\\n\")"]},{"cell_type":"markdown","metadata":{"id":"s9hzqvCKgol5"},"source":["# Parallel Processing of Multiple Samples"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4jFmg-F1t9EC","outputId":"3c41aefa-55bb-49a9-a591-4ae9f65f9aa4"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Processing batch 1 of 24 with 6 samples...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-17-aa2ab59ffc05>:52: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_filtered['ReadLengthBin'] = pd.cut(df_filtered['AdjustedReadLength'], bins=bins, include_lowest=True)\n","<ipython-input-17-aa2ab59ffc05>:55: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n","  grouped = df_filtered.groupby('ReadLengthBin')\n"]},{"output_type":"stream","name":"stdout","text":["Processing sample: SRR6195892\n","Processing sample: SRR5356297\n","Processing sample: SRR1531494\n","Processing sample: SRR1955042\n","Processing sample: SRR21391238\n","Processing sample: SRR960412\n","\n","\n","\n","\n","\n","\n","Input FASTQ file not found for SRR1955042\n","Downloaded files for SRR1531494:\n","\n","Processing original sample without truncation, adjusted read length: 50\n","Downloaded files for SRR6195892:\n","\n","Processing original sample without truncation, adjusted read length: 50\n","fastp output for SRR1531494 at length 50:\n","\n","fastp stderr for SRR1531494 at length 50:\n","Detecting adapter sequence for read1...\n","No adapter detected for read1\n","\n","Read1 before filtering:\n","total reads: 35757452\n","total bases: 1787872600\n","Q20 bases: 1759906961(98.4358%)\n","Q30 bases: 1708567549(95.5643%)\n","\n","Read1 after filtering:\n","total reads: 35586822\n","total bases: 1779341100\n","Q20 bases: 1755963157(98.6861%)\n","Q30 bases: 1705248685(95.836%)\n","\n","Filtering result:\n","reads passed filter: 35586822\n","reads failed due to low quality: 170630\n","reads failed due to too many N: 0\n","reads failed due to too short: 0\n","reads with adapter trimmed: 0\n","bases trimmed due to adapters: 0\n","\n","Duplication rate (may be overestimated since this is SE data): 61.9084%\n","\n","JSON report: /content/SRR1531494/fastp_report_50.json\n","HTML report: fastp.html\n","\n","fastp -i /content/SRR1531494/SRR1531494_1.fastq.gz -o /content/SRR1531494/SRR1531494_fastp_processed.fastq.gz -j /content/SRR1531494/fastp_report_50.json \n","fastp v0.23.4, time used: 143 seconds\n","\n","Total bases (depth) after filtering: 1779341100\n","Depth in Mbases: 1779.34 Mb\n","fastp output for SRR6195892 at length 50:\n","\n","fastp stderr for SRR6195892 at length 50:\n","Detecting adapter sequence for read1...\n","No adapter detected for read1\n","\n","Read1 before filtering:\n","total reads: 36267031\n","total bases: 1813351550\n","Q20 bases: 1786112951(98.4979%)\n","Q30 bases: 1740798256(95.9989%)\n","\n","Read1 after filtering:\n","total reads: 36133404\n","total bases: 1806670200\n","Q20 bases: 1782978953(98.6887%)\n","Q30 bases: 1738183709(96.2092%)\n","\n","Filtering result:\n","reads passed filter: 36133404\n","reads failed due to low quality: 133627\n","reads failed due to too many N: 0\n","reads failed due to too short: 0\n","reads with adapter trimmed: 0\n","bases trimmed due to adapters: 0\n","\n","Duplication rate (may be overestimated since this is SE data): 66.2186%\n","\n","JSON report: /content/SRR6195892/fastp_report_50.json\n","HTML report: fastp.html\n","\n","fastp -i /content/SRR6195892/SRR6195892_1.fastq.gz -o /content/SRR6195892/SRR6195892_fastp_processed.fastq.gz -j /content/SRR6195892/fastp_report_50.json \n","fastp v0.23.4, time used: 145 seconds\n","\n","Total bases (depth) after filtering: 1806670200\n","Depth in Mbases: 1806.67 Mb\n","Downloaded files for SRR21391238:\n","\n","Processing original sample without truncation, adjusted read length: 50\n","Picard CollectRnaSeqMetrics completed successfully for SRR1531494 at length 50\n","Copied RNA-Seq metrics to Google Drive: /content/drive/My Drive/Colab/Bryan_Colab/SRR1531494_50_Picard_RNA_Metrics.txt\n","Skipping Salmon quantification for SRR1531494 at length 50\n","Regions have been converted to BED format and saved to /content/SRR1531494/regions_from_other_dataset_50.bed\n","Sequences with coverage data saved to /content/SRR1531494/SRR1531494_50_enhanced_sequences_with_coverage.fa\n","Adjusted headers saved to /content/SRR1531494/SRR1531494_50_enhanced_sequences_with_coverage_corrected.fa\n","Copied corrected FASTA to Google Drive: /content/drive/My Drive/Colab/Bryan_Colab/SRR1531494_readlength_50_depth_1779.34Mb.fa\n","Deleted intermediate files for length 50\n","Deleted original FASTQ files for SRR1531494\n","Deleted sample directory /content/SRR1531494 to free up storage.\n","\n","All processing steps completed for SRR1531494.\n","Data preprocessing took 3798.68 seconds\n","\n","fastp output for SRR21391238 at length 50:\n","\n","fastp stderr for SRR21391238 at length 50:\n","Detecting adapter sequence for read1...\n","No adapter detected for read1\n","\n","Read1 before filtering:\n","total reads: 29351317\n","total bases: 2935131700\n","Q20 bases: 2881327102(98.1669%)\n","Q30 bases: 2790586278(95.0753%)\n","\n","Read1 after filtering:\n","total reads: 29351317\n","total bases: 2935131700\n","Q20 bases: 2881327102(98.1669%)\n","Q30 bases: 2790586278(95.0753%)\n","\n","Filtering result:\n","reads passed filter: 29351317\n","reads failed due to low quality: 0\n","reads failed due to too many N: 0\n","reads failed due to too short: 0\n","reads with adapter trimmed: 0\n","bases trimmed due to adapters: 0\n","\n","Duplication rate (may be overestimated since this is SE data): 49.7968%\n","\n","JSON report: /content/SRR21391238/fastp_report_50.json\n","HTML report: fastp.html\n","\n","fastp -i /content/SRR21391238/SRR21391238_1.fastq.gz -o /content/SRR21391238/SRR21391238_fastp_processed.fastq.gz -j /content/SRR21391238/fastp_report_50.json \n","fastp v0.23.4, time used: 154 seconds\n","\n","Total bases (depth) after filtering: 2935131700\n","Depth in Mbases: 2935.13 Mb\n","Picard CollectRnaSeqMetrics completed successfully for SRR6195892 at length 50\n","Copied RNA-Seq metrics to Google Drive: /content/drive/My Drive/Colab/Bryan_Colab/SRR6195892_50_Picard_RNA_Metrics.txt\n","Skipping Salmon quantification for SRR6195892 at length 50\n","Regions have been converted to BED format and saved to /content/SRR6195892/regions_from_other_dataset_50.bed\n","Sequences with coverage data saved to /content/SRR6195892/SRR6195892_50_enhanced_sequences_with_coverage.fa\n","Adjusted headers saved to /content/SRR6195892/SRR6195892_50_enhanced_sequences_with_coverage_corrected.fa\n","Copied corrected FASTA to Google Drive: /content/drive/My Drive/Colab/Bryan_Colab/SRR6195892_readlength_50_depth_1806.67Mb.fa\n","Deleted intermediate files for length 50\n","Deleted original FASTQ files for SRR6195892\n","Deleted sample directory /content/SRR6195892 to free up storage.\n","\n","All processing steps completed for SRR6195892.\n","Data preprocessing took 4373.92 seconds\n","\n","Downloaded files for SRR5356297:\n","\n","Processing original sample without truncation, adjusted read length: 51\n","Downloaded files for SRR960412:\n","\n","Processing original sample without truncation, adjusted read length: 51\n","fastp output for SRR5356297 at length 51:\n","\n","fastp stderr for SRR5356297 at length 51:\n","Detecting adapter sequence for read1...\n","No adapter detected for read1\n","\n","Read1 before filtering:\n","total reads: 63916702\n","total bases: 3259751802\n","Q20 bases: 3207192447(98.3876%)\n","Q30 bases: 3097638279(95.0268%)\n","\n","Read1 after filtering:\n","total reads: 63773151\n","total bases: 3252430701\n","Q20 bases: 3203620314(98.4993%)\n","Q30 bases: 3094581763(95.1467%)\n","\n","Filtering result:\n","reads passed filter: 63773151\n","reads failed due to low quality: 138032\n","reads failed due to too many N: 5519\n","reads failed due to too short: 0\n","reads with adapter trimmed: 0\n","bases trimmed due to adapters: 0\n","\n","Duplication rate (may be overestimated since this is SE data): 66.32%\n","\n","JSON report: /content/SRR5356297/fastp_report_51.json\n","HTML report: fastp.html\n","\n","fastp -i /content/SRR5356297/SRR5356297_1.fastq.gz -o /content/SRR5356297/SRR5356297_fastp_processed.fastq.gz -j /content/SRR5356297/fastp_report_51.json \n","fastp v0.23.4, time used: 216 seconds\n","\n","Total bases (depth) after filtering: 3252430701\n","Depth in Mbases: 3252.43 Mb\n","fastp output for SRR960412 at length 51:\n","\n","fastp stderr for SRR960412 at length 51:\n","Detecting adapter sequence for read1...\n","No adapter detected for read1\n","\n","Read1 before filtering:\n","total reads: 71579131\n","total bases: 3650535681\n","Q20 bases: 3546480179(97.1496%)\n","Q30 bases: 3363514011(92.1375%)\n","\n","Read1 after filtering:\n","total reads: 71233492\n","total bases: 3632908092\n","Q20 bases: 3538376200(97.3979%)\n","Q30 bases: 3357020651(92.4059%)\n","\n","Filtering result:\n","reads passed filter: 71233492\n","reads failed due to low quality: 345060\n","reads failed due to too many N: 579\n","reads failed due to too short: 0\n","reads with adapter trimmed: 0\n","bases trimmed due to adapters: 0\n","\n","Duplication rate (may be overestimated since this is SE data): 64.9782%\n","\n","JSON report: /content/SRR960412/fastp_report_51.json\n","HTML report: fastp.html\n","\n","fastp -i /content/SRR960412/SRR960412_1.fastq.gz -o /content/SRR960412/SRR960412_fastp_processed.fastq.gz -j /content/SRR960412/fastp_report_51.json \n","fastp v0.23.4, time used: 1013 seconds\n","\n","Total bases (depth) after filtering: 3632908092\n","Depth in Mbases: 3632.91 Mb\n","Picard CollectRnaSeqMetrics completed successfully for SRR21391238 at length 50\n","Copied RNA-Seq metrics to Google Drive: /content/drive/My Drive/Colab/Bryan_Colab/SRR21391238_50_Picard_RNA_Metrics.txt\n","Skipping Salmon quantification for SRR21391238 at length 50\n","Regions have been converted to BED format and saved to /content/SRR21391238/regions_from_other_dataset_50.bed\n","Sequences with coverage data saved to /content/SRR21391238/SRR21391238_50_enhanced_sequences_with_coverage.fa\n","Adjusted headers saved to /content/SRR21391238/SRR21391238_50_enhanced_sequences_with_coverage_corrected.fa\n","Copied corrected FASTA to Google Drive: /content/drive/My Drive/Colab/Bryan_Colab/SRR21391238_readlength_50_depth_2935.13Mb.fa\n","Deleted intermediate files for length 50\n","Deleted original FASTQ files for SRR21391238\n","Deleted sample directory /content/SRR21391238 to free up storage.\n","\n","All processing steps completed for SRR21391238.\n","Data preprocessing took 7422.32 seconds\n","\n","Picard CollectRnaSeqMetrics completed successfully for SRR5356297 at length 51\n","Copied RNA-Seq metrics to Google Drive: /content/drive/My Drive/Colab/Bryan_Colab/SRR5356297_51_Picard_RNA_Metrics.txt\n","Skipping Salmon quantification for SRR5356297 at length 51\n","Regions have been converted to BED format and saved to /content/SRR5356297/regions_from_other_dataset_51.bed\n","Picard CollectRnaSeqMetrics completed successfully for SRR960412 at length 51\n","Copied RNA-Seq metrics to Google Drive: /content/drive/My Drive/Colab/Bryan_Colab/SRR960412_51_Picard_RNA_Metrics.txt\n","Skipping Salmon quantification for SRR960412 at length 51\n","Regions have been converted to BED format and saved to /content/SRR960412/regions_from_other_dataset_51.bed\n","Sequences with coverage data saved to /content/SRR5356297/SRR5356297_51_enhanced_sequences_with_coverage.fa\n","Adjusted headers saved to /content/SRR5356297/SRR5356297_51_enhanced_sequences_with_coverage_corrected.fa\n","Copied corrected FASTA to Google Drive: /content/drive/My Drive/Colab/Bryan_Colab/SRR5356297_readlength_51_depth_3252.43Mb.fa\n","Deleted intermediate files for length 51\n","Deleted original FASTQ files for SRR5356297\n","Deleted sample directory /content/SRR5356297 to free up storage.\n","\n","All processing steps completed for SRR5356297.\n","Data preprocessing took 7550.87 seconds\n","\n","Sequences with coverage data saved to /content/SRR960412/SRR960412_51_enhanced_sequences_with_coverage.fa\n","Adjusted headers saved to /content/SRR960412/SRR960412_51_enhanced_sequences_with_coverage_corrected.fa\n","Copied corrected FASTA to Google Drive: /content/drive/My Drive/Colab/Bryan_Colab/SRR960412_readlength_51_depth_3632.91Mb.fa\n","Deleted intermediate files for length 51\n","Deleted original FASTQ files for SRR960412\n","Deleted sample directory /content/SRR960412 to free up storage.\n","\n","All processing steps completed for SRR960412.\n","Data preprocessing took 7634.75 seconds\n","\n","Batch 1 completed in 7634.85 seconds.\n","\n","\n","Processing batch 2 of 24 with 6 samples...\n","Processing sample: SRR18888647\n","Processing sample: SRR19688199\n","Processing sample: SRR27286278\n","Processing sample: SRR26388431\n","Processing sample: SRR27286279\n","Processing sample: SRR26548228\n","\n","\n","\n","\n","\n","\n"]}],"source":["if __name__ == '__main__':\n","    import multiprocessing\n","    import time\n","    from math import ceil\n","    import pandas as pd\n","    import numpy as np\n","\n","    # ===== Parameter Configurations =====\n","    # Flags\n","    filter_sequences = False       # Set to False to skip filtering\n","    truncate = False                # Set to True or False\n","    run_star = True\n","    run_picard = True\n","    run_salmon = False\n","\n","    # Truncation parameters\n","    truncation_range = (50, 150)    # Set the truncation range if truncate is True\n","    truncation_step = 25            # Set the truncation step if truncate is True\n","\n","    # Threading\n","    threads_per_sample = 16         # Adjust based on your environment\n","\n","    # File paths\n","    regions_fasta_file = \"/content/drive/My Drive/Colab/Bryan_Colab/SRR10072432_1_filtered_high_quality_sequences_150bp.fa\"\n","    drive_metrics_dir = '/content/drive/My Drive/Colab/Bryan_Colab'\n","\n","    # Sampling parameters\n","    csv_path = '/content/drive/My Drive/Colab/Bryan_Colab/srr_metadata_scrape_filtered.csv'\n","    num_samples = 160                 # You can adjust this number as needed\n","\n","    # Binning parameters\n","    num_bins = 21                    # Number of bins between 50 and 150 (if you want 2 bins -> value needs to be 3)\n","    bin_start, bin_end = 50, 150\n","    bins = np.linspace(bin_start, bin_end, num=num_bins+1)  # e.g., 11 edges for 10 bins\n","\n","    # ===== Load and Process CSV =====\n","    df = pd.read_csv(csv_path)\n","\n","    # Adjust ReadLength for paired-end reads\n","    def adjust_read_length(row):\n","        if row['LibraryLayout'].upper() == 'PAIRED':\n","            return row['ReadLength'] / 2\n","        else:\n","            return row['ReadLength']\n","\n","    df['AdjustedReadLength'] = df.apply(adjust_read_length, axis=1)\n","\n","    # Filter samples to AdjustedReadLength between 50 and 150\n","    df_filtered = df[(df['AdjustedReadLength'] >= bin_start) & (df['AdjustedReadLength'] <= bin_end)]\n","\n","    # Assign bins\n","    df_filtered['ReadLengthBin'] = pd.cut(df_filtered['AdjustedReadLength'], bins=bins, include_lowest=True)\n","\n","    # Now, group by 'ReadLengthBin' and randomly sample from each bin\n","    grouped = df_filtered.groupby('ReadLengthBin')\n","\n","    samples_per_bin = max(1, num_samples // len(grouped))  # Ensure at least one sample per bin\n","\n","    sampled_dfs = []\n","\n","    for name, group in grouped:\n","        n = min(samples_per_bin, len(group))  # Don't sample more than available\n","        sampled_dfs.append(group.sample(n=n, random_state=42))\n","\n","    sampled_df = pd.concat(sampled_dfs)\n","\n","    # If we have more samples than needed, randomly select the required number\n","    if len(sampled_df) > num_samples:\n","        sampled_df = sampled_df.sample(n=num_samples, random_state=42)\n","\n","    # Prepare a list of arguments for process_sample\n","    process_args = []\n","\n","    for idx, row in sampled_df.iterrows():\n","        sample = row['Run']\n","        adjusted_read_length = row['AdjustedReadLength']\n","        library_layout = row['LibraryLayout']\n","        process_args.append((\n","            sample,\n","            adjusted_read_length,\n","            library_layout,\n","            filter_sequences,\n","            truncate,\n","            truncation_range,\n","            truncation_step,\n","            run_star,\n","            run_picard,\n","            run_salmon,\n","            threads_per_sample,\n","            regions_fasta_file,\n","            drive_metrics_dir\n","        ))\n","\n","    # Number of parallel processes (adjust based on available cores and desired threads per sample)\n","    cpu_available = multiprocessing.cpu_count()\n","    num_processes = min(len(process_args), cpu_available // threads_per_sample)\n","    num_processes = max(1, num_processes)  # Ensure at least one process\n","\n","    # Function to batch samples\n","    def batch_samples(args_list, batch_size):\n","        \"\"\"Yield successive batches from args_list.\"\"\"\n","        for i in range(0, len(args_list), batch_size):\n","            yield args_list[i:i + batch_size]\n","\n","    # Process samples in batches\n","    total_batches = ceil(len(process_args) / num_processes)\n","    batch_number = 1\n","\n","    for arg_batch in batch_samples(process_args, num_processes):\n","        print(f\"\\nProcessing batch {batch_number} of {total_batches} with {len(arg_batch)} samples...\")\n","        start_batch_time = time.time()\n","\n","        # Create a pool of worker processes\n","        pool = multiprocessing.Pool(processes=len(arg_batch))\n","\n","        # Map the process_sample function to the batch of samples with the new parameters\n","        pool.starmap(process_sample, arg_batch)\n","\n","        # Close the pool and wait for the work to finish\n","        pool.close()\n","        pool.join()\n","\n","        end_batch_time = time.time()\n","        print(f\"Batch {batch_number} completed in {end_batch_time - start_batch_time:.2f} seconds.\\n\")\n","        batch_number += 1"]},{"cell_type":"code","source":["from IPython.display import display, Javascript\n","\n","# This command disconnects the current runtime\n","display(Javascript('google.colab.kernel.disconnect()'))\n"],"metadata":{"id":"23qCCWFlba9d","executionInfo":{"status":"aborted","timestamp":1730285082309,"user_tz":-60,"elapsed":5,"user":{"displayName":"Neal Amin","userId":"01047124843631517737"}}},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyOLgH91fJbuvZRi4Mc5MxoD"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}